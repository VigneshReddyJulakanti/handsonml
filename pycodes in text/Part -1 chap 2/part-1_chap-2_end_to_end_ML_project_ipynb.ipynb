{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For getting the housing data\n",
    "import os\n",
    "import tarfile\n",
    "from matplotlib.colors import hexColorPattern\n",
    "from numpy.lib.function_base import median\n",
    "from numpy.lib.histograms import _histogram_bin_edges_dispatcher\n",
    "# from six.moves import urllib\n",
    "\n",
    "\n",
    "# for reading the csv file\n",
    "import pandas as pd\n",
    "\n",
    "#For plotting the graph\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for generating random numbers\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "#When we want to get the housing data\n",
    "\n",
    "\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    " if not os.path.isdir(housing_path):\n",
    "    os.makedirs(housing_path)\n",
    " tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    " urllib.request.urlretrieve(housing_url, tgz_path)\n",
    " housing_tgz = tarfile.open(tgz_path)\n",
    " housing_tgz.extractall(path=housing_path)\n",
    " housing_tgz.close()\n",
    "\n",
    "fetch_housing_data()\n",
    "\n",
    " '''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Ploting the Data\n",
    "\n",
    "housing_data=pd.read_csv(\"datasets\\housing\\housing.csv\")\n",
    "\n",
    "\n",
    "\n",
    "# print(housing_data.head())    #Prints the first five rows\n",
    "# print(housing_data.info())   # Gives information about the csv file\n",
    "# print(housing_data[\"ocean_proximity\"].value_counts())      #GIves the value_count\n",
    "# print(housing_data.describe()) #Gives information of max min etc , attributes whose values are integers\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "housing_data.hist(bins=50)  # Here bins is how many parts the histogram to be divided into  or simple bins are the no of bars\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "# housing_data[\"median_income\"].hist()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "housing_data[\"income_cat\"]=pd.cut(housing_data[\"median_income\"],bins=[0,1.5,3,4.5,6.,np.inf],labels=[1,2,3,4,5]) # np.inf mean inserting positive infinity , -np.inf means negative infinity\n",
    "# housing_data[\"income_cat\"].hist()\n",
    "# plt.show()\n",
    "\n",
    "# we are making a new column named income_cat in housing_data DataFrame which contains the different categories of median_income\n",
    "# i.e we are dividing the median_income into 5 categories , if median_income is from 0 to 1.5 it is named a , 1.5 to 3.0 named b , so on 6 to np.inf i.e 6 to positive infinity named as e .\n",
    "# housing_data[\"income_cat\"]=pd.cut(housing_data[\"median_income\"],bins=[0,1.5,3,4.5,6.,np.inf],labels=[\"a\",\"b\",\"c\",\"d\",\"e\"])\n",
    "# print(housing_data)\n",
    "# print(housing_data[\"income_cat\"].hist())\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# housing_data[\"income_cat\"]=pd.cut(housing_data[\"median_income\"],bins=[0,1.5,3,4.5,6.,np.inf],labels=[5,10,15,20,25])\n",
    "# housing_data[\"income_cat\"].hist()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# housing_data[\"income_cat\"]=pd.cut(housing_data[\"median_income\"],bins=[0,1.5,3,4.5,6.,np.inf],labels=[100,200,300,400,500])\n",
    "# housing_data[\"income_cat\"].hist()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Creating a test set on own code\n",
    "\n",
    "\n",
    "def split_training_set(dataset,test_ratio):\n",
    "      np.random.seed(45)            #This is used to generate the same ordered random numbers every time\n",
    "      shuffled_indices=np.random.permutation(len(dataset))   #will create a new list of shuffled indices\n",
    "      test_set_size=int(len(dataset)*test_ratio)\n",
    "      test_set_indices=shuffled_indices[:test_set_size]\n",
    "      train_set_indices=shuffled_indices[test_set_size:]\n",
    "      return dataset.iloc[train_set_indices],dataset.iloc[test_set_indices]\n",
    "   \n",
    "train_set,test_set=split_training_set(housing_data,0.2)\n",
    "print(test_set)\n",
    "print(train_set)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Making train_set and test set using the predefined function available from sklearn.model_selection import train_test_split\n",
    "# train_set,test_set=train_test_split(housing_data,test_size=0.2,random_state=40)\n",
    "# print(train_set)\n",
    "# print(test_set)\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "# Stratified split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "a=StratifiedShuffleSplit(n_splits=1,test_size=0.2,random_state=42)\n",
    "for  str_train_set_index,str_test_set_index in a.split(housing_data,housing_data[\"income_cat\"]):\n",
    "      str_train_set=housing_data.loc[str_train_set_index]\n",
    "      # print(str_test_set_index)\n",
    "      str_test_set=housing_data.loc[str_test_set_index]\n",
    "#check\n",
    "# print(str_test_set[\"income_cat\"].value_counts()/len(str_test_set))\n",
    "\n",
    "for boom in ( str_test_set,str_train_set):\n",
    "      boom.drop(\"income_cat\",axis=1,inplace=True)\n",
    "\n",
    "#Making a copy ,so not to disturb the original data\n",
    "housing=str_train_set.copy()\n",
    "\n",
    "# housing.plot(kind=\"scatter\" ,x=\"longitude\", y=\"latitude\")\n",
    "# housing.plot(kind=\"scatter\" ,x=\"longitude\", y=\"latitude\",alpha=0.1)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "#By default colour bar will be True\n",
    "#s means the radius of bubbles may be in mm\n",
    "# label means label for the bubble\n",
    "# c means which property should determine colour\n",
    "#we are using cmap template for colour\n",
    "# housing.plot(kind=\"scatter\",x=\"longitude\", y=\"latitude\" , s=housing[\"population\"]/100 , label=\"population\" , figsize=(10,7),c=\"median_house_value\",cmap=plt.get_cmap(\"jet\"),colorbar=True ) \n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# # # correlation\n",
    "# corr_matrix=housing.corr()\n",
    "# print(corr_matrix[\"median_house_value\"].sort_values(ascending=False))\n",
    "\n",
    "\n",
    "# # scatter matrix to get correlation graph\n",
    "# from pandas.plotting import scatter_matrix\n",
    "# attributes=[\"median_house_value\",\"median_income\", \"total_rooms\",\"housing_median_age\"]\n",
    "# scatter_matrix(housing[attributes],figsize=(12,8))\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "## two plot the graph between the properties having the best correlation , i.e median_income and median_house_value so we can observe the straight lines in the data , so we can remove the data that gives the staright line which confuses the algorithm .\n",
    "# housing.plot(kind=\"scatter\" , x=\"median_income\" , y=\"median_house_value\" , alpha=0.1)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# #creating new data using the pre existing data \n",
    "# housing[\"rooms_per_household\"]=housing[\"total_rooms\"]/housing[\"households\"]\n",
    "# housing[\"population_per_household\"]=housing[\"population\"]/housing[\"households\"]\n",
    "# housing[\"bedrooms_per_room\"]=housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\n",
    "\n",
    "# corr_matrix=housing.corr()\n",
    "# print(corr_matrix[\"median_house_value\"].sort_values(ascending=False))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################## Preparing data for machine learning algorithm ##################################\n",
    "\n",
    "#lets create a new data set for cleaning \n",
    "#lets create or change the housing data set and give it the data set of stratified data set with out median_house_value\n",
    "#lets take median_house_value as label\n",
    "housing=str_train_set.drop(\"median_house_value\",axis=1)  #This will copy the data of str_train_set to housing by removing median_house_value # axis = 1 indicates to remove the y axis\n",
    "housing_label=str_train_set[\"median_house_value\"].copy()\n",
    "# print(housing_label)\n",
    "\n",
    "# print(housing.info())# from this we can observe that the total_bedrooms value is missing in some rows\n",
    "\n",
    "# #now we have 3 options\n",
    "# #1) Remove the total_bedroom attribute\n",
    "# housing.drop(\"total_bedroom\",axis=1,inplace=True)\n",
    "# #2) Remove the rows that are empty\n",
    "# housing.dropna(subset=[\"total_bedrooms\"],inplace=True)\n",
    "# #3) To fill the empty cells with the median value or mean or zero value\n",
    "# median=housing[\"total_bedrooms\"].median()\n",
    "# housing[\"total_bedrooms\"].fillna(median,inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "#Imputer method to fill the cells that are empty\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "housing_num=housing.drop(\"ocean_proximity\",axis=1)\n",
    "# print(housing.info())\n",
    "# print(housing_num.info())\n",
    "\n",
    "imputer=SimpleImputer(strategy=\"median\")\n",
    "imputer.fit(housing_num)\n",
    "# print(imputer.statistics_) #All the median values are stored in imputer.statistics_\n",
    "x=imputer.transform(housing_num)\n",
    "## print(x)\n",
    "##now we have x in the form of an numpy array and with no columns lets transform it back into dataframe\n",
    "housing_tr=pd.DataFrame(x,columns=housing_num.columns)\n",
    "# print(housing_tr.info()) #Now check , we can observe that all the cells are full\n",
    "\n",
    "\n",
    "## Handling Text and Categorial Attributes\n",
    "housing_cat=housing[[\"ocean_proximity\"]]\n",
    "# print(housing_cat.head(10))\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "ordinal_encoder=OrdinalEncoder()\n",
    "# housing_cat_encoder=ordinal_encoder.fit_transform(housing_cat)\n",
    "# print(housing_cat_encoder[:10])\n",
    "# print(ordinal_encoder.categories_)\n",
    "\n",
    "#One HOt encoder\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "cat_encoder=OneHotEncoder()\n",
    "# housing_cat_1hot=cat_encoder.fit_transform(housing_cat) #Now this will be in SciPy sparse matrix.\n",
    "# print(housing_cat_1hot)\n",
    "# housing_cat_1hot=housing_cat_1hot.toarray()\n",
    "# print(housing_cat_1hot)\n",
    "\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "from sklearn.base import BaseEstimator,TransformerMixin\n",
    "rooms_ix,bedrooms_ix,population_ix,households_ix=3,4,5,6   #These are the indexes of the specified columns in the data frame or csv\n",
    "\n",
    "class CombinedAtrributesAdder(BaseEstimator,TransformerMixin):\n",
    "      def __init__(self,add_bedrooms_per_room=True):\n",
    "            self.add_bedrooms_per_room=add_bedrooms_per_room\n",
    "      def fit(self,X,y=None):\n",
    "            return self\n",
    "      def transform(self,X,y=None):\n",
    "            rooms_per_household=X[:,rooms_ix]/X[:,households_ix]   #we are slicing a multi dimensional array , here we are saying to take all the values in first dimension that is all rows and we are specifying second dimension after comma , i.e here we are saying which column to select\n",
    "            population_per_household=X[:,population_ix]/X[:,households_ix]\n",
    "            if self.add_bedrooms_per_room:\n",
    "                  bedrooms_per_room=X[:,bedrooms_ix]/X[:,rooms_ix]\n",
    "                  return np.c_[X,rooms_per_household,population_per_household,bedrooms_per_room]  #np.c_ will concatenate all the lists given inside it in side by side manner\n",
    "            else:\n",
    "                  return np.c_[X,rooms_per_household,population_per_household]\n",
    "\n",
    "attr_adder=CombinedAtrributesAdder()\n",
    "housing_extra_attribs=attr_adder.transform(housing.values)\n",
    "# print(housing_extra_attribs)\n",
    "# print(type(housing.values)) #It is a numpy array\n",
    "\n",
    "#Transformation pipelines \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "num_pipeline=Pipeline([\n",
    "      (\"imputer\",SimpleImputer(strategy=\"median\")),\n",
    "      (\"attribs_adder\",CombinedAtrributesAdder()),\n",
    "      (\"std_scalar\",StandardScaler())\n",
    "])\n",
    "housing_num_tr=num_pipeline.fit_transform(housing_num)\n",
    "# print(list(housing_num))   # This gives the list of indexes of housing num\n",
    "\n",
    "#column transformer i.e it will transform both the categorial and the numerical columns in a single pipeline and will again combine it\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "num_attributes=list(housing_num)\n",
    "cat_attributes=[\"ocean_proximity\"]\n",
    "full_pipeline=ColumnTransformer([\n",
    "      (\"num\",num_pipeline,num_attributes),\n",
    "      (\"Car\",OneHotEncoder(),cat_attributes)\n",
    "])\n",
    "housing_prepared=full_pipeline.fit_transform(housing)\n",
    "# print(housing_prepared)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #select and train a model\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# lin_reg=LinearRegression()\n",
    "# lin_reg.fit(housing_prepared,housing_label)\n",
    "\n",
    "# # some_data=housing.iloc[:5]\n",
    "# # # print(f\"some_data = {some_data}\")\n",
    "# # some_labels=housing_label.iloc[:5]\n",
    "# # # print(f\"some labels = {some_labels}\")\n",
    "# # # print(f\"some labels = {list(some_labels)}\")\n",
    "# # some_data_prepared=full_pipeline.transform(some_data)\n",
    "# # # print(f\"some_data = {some_data_prepared}\")\n",
    "\n",
    "# # # print(\"predictions: \",lin_reg.predict(some_data_prepared))\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# housing_predictions=lin_reg.predict(housing_prepared)\n",
    "# lin_mse=mean_squared_error(housing_label,housing_predictions)\n",
    "# lin_rmse=np.sqrt(lin_mse)\n",
    "# print(\"lin_rmse\",lin_rmse)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THis is continued from the part 1 file , line no 343 Total code is in the above cell till line 343\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "# # from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# tree_reg=DecisionTreeRegressor()\n",
    "# tree_reg.fit(housing_prepared,housing_label)\n",
    "# housing_predictions=tree_reg.predict(housing_prepared)\n",
    "# tree_mse=mean_squared_error(housing_predictions,housing_label)\n",
    "# tree_rmse=np.sqrt(tree_mse)\n",
    "# tree_mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "# scores=cross_val_score(tree_reg,housing_prepared,housing_label,scoring=\"neg_mean_squared_error\",cv=10)\n",
    "# tree_rmse_scores=np.sqrt(-scores)\n",
    "# # print(\"tree_rmse_scores\",tree_rmse_scores)\n",
    "# # print(\"Scores:\",scores)\n",
    "\n",
    "def display_scores(scores):\n",
    "    print(\"scores:\",scores)\n",
    "    print(\"mean:\",scores.mean())\n",
    "    print(\"standard_deviation\",scores.std())\n",
    "\n",
    "# display_scores(tree_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores: [49643.43314941 47624.41848451 50025.22022011 52201.78280485\n",
      " 49413.2645514  53402.02666519 48896.64706447 47981.6819675\n",
      " 52923.83241384 50147.34350448]\n",
      "mean: 50225.96508257634\n",
      "standard_deviation 1894.4116300325948\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "forest_reg=RandomForestRegressor()\n",
    "forest_reg.fit(housing_prepared,housing_label)\n",
    "# scores=cross_val_score(forest_reg,housing_prepared,housing_label,scoring=\"neg_mean_squared_error\",cv=10)\n",
    "# scores=np.sqrt(-scores)\n",
    "# display_scores(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48372.11440276615"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test=str_test_set.drop(\"median_house_value\",axis=1)\n",
    "Y_test=str_test_set[\"median_house_value\"].copy()   \n",
    "X_test_prepared=full_pipeline.transform(X_test)\n",
    "final_prediction=forest_reg.predict(X_test_prepared)\n",
    "final_mse=mean_squared_error(Y_test,final_prediction)\n",
    "final_rmse=np.sqrt(final_mse)\n",
    "final_rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ee5cc6fef2d70a7e71ee3826687cbd150f18158e0b1eef11d4f4f92bb920e304"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
